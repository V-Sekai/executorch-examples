# ExecuTorch Multi-Backend Linear Regression Demo

A comprehensive demonstration of ExecuTorch's cross-platform capabilities, showcasing linear regression inference across multiple backends: **XNNPACK** (CPU), **Vulkan** (GPU), and **MPS** (Apple Metal).

## ğŸš€ Quick Start

### 1. Export Models for All Backends
```bash
cd linear_regression/python
python export_multi_backend.py
```

### 2. Run Python Benchmark
```bash
python benchmark.py
```

### 3. Build and Run C++ Demo
```bash
cd ../cpp
chmod +x build_multi.sh
./build_multi.sh
./../../build_multi/bin/executorch_multi_backend_demo
```

## ğŸ—ï¸ Architecture

### Backend Support
- **ğŸ MPS (Apple Metal)**: Hardware-accelerated inference on Apple Silicon
- **ğŸ® Vulkan**: Cross-platform GPU compute acceleration  
- **ğŸ“± XNNPACK**: Highly optimized CPU kernels
- **ğŸ”§ Portable**: Reference implementation (no optimization)

### Smart Fallback System
The demo automatically detects available hardware and falls back gracefully:
```
MPS (Apple) â†’ Vulkan (GPU) â†’ XNNPACK (CPU) â†’ Portable
```

## ğŸ“Š Features

### Python Tools
- **`export_multi_backend.py`**: Exports models for all supported backends
- **`benchmark.py`**: Performance comparison across backends
- Automatic platform detection and fallback model generation

### C++ Demo Application  
- **Multi-backend testing**: Automatically tests all available backends
- **Performance benchmarking**: 1000-iteration timing with warmup
- **Intelligent ranking**: Shows performance hierarchy
- **Cross-platform compatibility**: Builds on Linux, macOS, and Windows

### Example Output
```
ğŸš€ ExecuTorch Multi-Backend Linear Regression Demo
=======================================================
ğŸ“Š Input features: [1, 2, 3, 4]

âœ… MPS (Apple Metal): 0.012 ms avg
âœ… Vulkan (GPU): 0.018 ms avg  
âœ… XNNPACK (CPU): 0.045 ms avg
âœ… Portable (Reference): 0.123 ms avg

ğŸ† Performance Ranking:
--------------------------------------------------
ğŸ¥‡ MPS (Apple Metal)         0.012 ms (output: 2.718281)
ğŸ¥ˆ Vulkan (GPU)              0.018 ms (output: 2.718281) 
ğŸ¥‰ XNNPACK (CPU)             0.045 ms (output: 2.718281)
ğŸ“Š Portable (Reference)      0.123 ms (output: 2.718281)

ğŸ“ˆ Summary:
   ğŸƒ Fastest: MPS (Apple Metal)
   âš¡ Speed improvement: 10.2x over slowest

âœ… ExecuTorch multi-backend demo completed successfully!
```

## ğŸ”§ Dependencies

### Python
- PyTorch 2.0+
- ExecuTorch 0.6.0
- Platform-specific backend packages (auto-detected)

### C++
- CMake 3.18+
- C++17 compatible compiler
- Vulkan SDK (optional, for GPU acceleration)

## ğŸ§ª CI/CD Testing

GitHub Actions workflow tests across:
- **Ubuntu**: XNNPACK + Vulkan
- **macOS**: MPS + XNNPACK  
- **Windows**: XNNPACK

## ğŸ¯ Use Cases

This demo showcases ExecuTorch's ability to:
- **Optimize for target hardware**: Automatically select the fastest backend
- **Provide consistent APIs**: Same inference code across all backends
- **Ensure portability**: Graceful fallback when specialized hardware unavailable
- **Deliver performance**: Up to 10x speedup with hardware acceleration

Perfect for applications requiring:
- Cross-platform deployment
- Hardware-agnostic inference
- Performance optimization
- Edge device compatibility

## ğŸ“ Project Structure
```
linear_regression/
â”œâ”€â”€ python/
â”‚   â”œâ”€â”€ export_multi_backend.py  # Multi-backend model export
â”‚   â”œâ”€â”€ benchmark.py             # Performance comparison
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ cpp/
â”‚   â”œâ”€â”€ main_multi_backend.cpp   # Multi-backend C++ demo
â”‚   â”œâ”€â”€ CMakeLists_multi.txt     # Build configuration
â”‚   â””â”€â”€ build_multi.sh           # Build script
â”œâ”€â”€ models/                      # Generated .pte files
â””â”€â”€ README.md                    # This file
```